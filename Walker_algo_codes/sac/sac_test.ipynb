{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f1ff4-b2dc-424c-bbee-581aee753151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca582a1-1f3b-4246-b297-932e7205be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600fd17-bff8-4263-a19e-ef9a8fd31cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample()\n",
    "        action = torch.tanh(mean+ std*z.to(device))\n",
    "        log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample().to(device)\n",
    "        action = torch.tanh(mean + std*z)\n",
    "        \n",
    "        action  = action.cpu()#.detach().cpu().numpy()\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bb922-e3ab-46e2-9008-d9501ee1b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Walker2d-v2\")\n",
    "\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "hidden_dim = 256\n",
    "# Define the neural network\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)# Define input_size and output_size accordingly\n",
    "\n",
    "# Load the saved state dictionary\n",
    "policy_net.load_state_dict(torch.load('policy_net.pth'))\n",
    "\n",
    "# Test the loaded model\n",
    "num_episodes = 100  # Number of evaluation episodes\n",
    "results=[]\n",
    "for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = policy_net.get_action(state).detach()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            cumulative_reward += reward\n",
    "            \n",
    "        results.append(cumulative_reward)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026a793-dd40-4965-ada8-3c591f6f6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Return for sac: {np.mean(results)}\")\n",
    "print(f\"Standard Deviation of Return for sac: {np.std(results)}\")\n",
    "print(f\"Variance of Return for sac: {np.var(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62cf1a-0f97-4d15-8e61-d2d95c062ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_scale = 0.1\n",
    "perturbed_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = policy_net.get_action(state).detach()\n",
    "        action += np.random.normal(0, perturbation_scale, size=action.shape)  # Perturb the action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    perturbed_rewards.append(cumulative_reward)\n",
    "\n",
    "perturbed_rewards = np.array(perturbed_rewards)\n",
    "print(f\"Average Return under Perturbation for sac: {np.mean(perturbed_rewards)}\")\n",
    "print(f\"Standard Deviation under Perturbation for sac: {np.std(perturbed_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97992804-abe7-4339-bd59-0a172ef415f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards= np.load('sac_reward.npy')\n",
    "times=np.load('sac_time.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8397d8-74f6-4162-a549-e59b9586ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_performance = np.mean(rewards[:int(len(rewards) * 0.1)])\n",
    "print(f\"Early Performance for sac: {early_performance}\")\n",
    "\n",
    "\n",
    "late_performance = rewards[-int(len(rewards) * 0.1):]\n",
    "variance_late_performance = np.var(late_performance)\n",
    "print(f\"Variance of Late Performance for sac: {variance_late_performance}\")\n",
    "\n",
    "\n",
    "total_time = sum(times)\n",
    "average_time_per_episode = np.mean(times)\n",
    "print(f\"Total Training Time for sac: {total_time} seconds\")\n",
    "print(f\"Average Time per Episode for sac: {average_time_per_episode} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce469d60-f084-4e25-9d91-73d2a662393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards[:1000])\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Episode vs Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10f3b7-4faf-49a6-922d-73a6bf2e5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Walker2d-v2')  \n",
    "frames = []\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "\n",
    "    frame = env.render(mode='rgb_array')\n",
    "    frames.append(frame)\n",
    "\n",
    "\n",
    "    action = policy_net.get_action(state).detach()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    cumulative_reward += reward\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "output_video_path = 'sac_output_video.avi'\n",
    "frame_height, frame_width, _ = frames[0].shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_writer = cv2.VideoWriter(output_video_path, fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "for frame in frames:\n",
    "    video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video saved to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f6b22-15db-41cf-bc22-87b4a97870d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a80a8f-ccaa-4781-80f4-b956cb08dfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
